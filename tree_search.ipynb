{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xAH0nvLoNTn"
      },
      "source": [
        "# Tree search with 0-1 integer programming\n",
        "\n",
        "This notebook follows from the meeting on 5 Oct, 2021. I think it's a good time to expand the action space and allow random flipping of variables. Before I get make myself crazy again, I want to make sure that it achieves something.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7GfgJYHtli-"
      },
      "source": [
        "I wish one day they can automatically included these for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhCTOQJGtod8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import gurobipy as gp \n",
        "from gurobipy import GRB\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3bFLlf3qNoT"
      },
      "source": [
        "## Gurobi to actually solve the 0-1 integer programs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-yWlW76qvH3"
      },
      "source": [
        "def gurobi_optimal(A, b, c, *, num_solution_limit = 32):\n",
        "    m, n = A.shape\n",
        "    assert b.shape == (m, ) and c.shape == (n, )\n",
        "\n",
        "    ip = gp.Model()\n",
        "    x_var = ip.addMVar(n, lb = 0.0, ub = float('inf'), vtype = GRB.BINARY, name = 'x')\n",
        "    ip.addConstr(A @ x_var <= b)\n",
        "    ip.setObjective(c @ x_var, GRB.MAXIMIZE)\n",
        "    ip.setParam('OutputFlag', 0)\n",
        "    ip.setParam('PoolSearchMode', 2)\n",
        "    ip.setParam('PoolSolutions', num_solution_limit)\n",
        "    ip.optimize()\n",
        "\n",
        "    solutions = []\n",
        "    obj = c @ np.round(x_var.x)\n",
        "    for i in range(ip.SolCount):\n",
        "        ip.setParam('SolutionNumber', i)\n",
        "        sol = np.array(ip.xn).round().astype(bool)\n",
        "        if c @ sol < obj:\n",
        "            break\n",
        "        else:\n",
        "            solutions.append(sol)\n",
        "    return obj, np.stack(solutions)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKThhWpJsc7X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ncCnLY4sdVI"
      },
      "source": [
        "## Data and dataset\n",
        "### Problems to be explored\n",
        "Please refer to the Dataset part for details.\n",
        "\n",
        "#### Set Cover Problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPIeIR4bsLgs"
      },
      "source": [
        "from problems.setcover import *\n",
        "problem_func = generate_setcover\n",
        "encoding_func = setcover_encoding\n",
        "def size_func():\n",
        "    return {'nrows': 100, 'ncols': 200, 'density': np.random.uniform(0.1, 0.25)}\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0rlaO539Gu_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhS8_QTAsmmr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6vQyn6PvVFN"
      },
      "source": [
        "#### Verify the problem size\n",
        "- $A\\in \\mathbb{R}^{m\\times n}$\n",
        "- $b\\in \\mathbb{R}^m$\n",
        "- $c\\in \\mathbb{R}^n$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwmeD98pvUH9"
      },
      "source": [
        "def valid_dim(A, b, c, x = None):\n",
        "    m, n = A.shape \n",
        "    return b.shape == (m, ) and c.shape == (n,) and (x is None or x.shape == (n, ))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6UlDt8rsm6L"
      },
      "source": [
        "### Encoding of the problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xL0XgfospHp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo4h6mANsp0h"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsEyGpKetWtw"
      },
      "source": [
        "class BinaryIPDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Generate a problem instance with a random, non-terminal assignment\n",
        "    '''\n",
        "    def __init__(self, *, problem_func, size_func, assignment_func, encoding_func):\n",
        "        '''\n",
        "        problem_func: problem to be explored\n",
        "        size_func: size as a input parameter to the problem_func\n",
        "        problem_func(**size_func()) should work\n",
        "        \n",
        "        assignment_func: used to generate an assignemnt for the given problem\n",
        "        encoding_func: \n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.problem_func = problem_func\n",
        "        self.size_func = size_func\n",
        "        self.assignment_func = assignment_func\n",
        "        self.encoding_func = encoding_func\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError # not needed\n",
        "\n",
        "    def __getitem__(self, index:int):\n",
        "        # get a random instance in the class of problem with a random size\n",
        "        A, b, c = self.problem_func(**self.size_func())\n",
        "\n",
        "        # get a meaningful but random variable assignemnt\n",
        "        x_assignment = self.assignment_func(A, b, c)\n",
        "        \n",
        "        # get a random intermediate solution\n",
        "        _, solutions = gurobi_optimal(A, b, c)\n",
        "\n",
        "        difference = np.logical_xor(x_assignment, solutions)\n",
        "        dist = difference.sum(axis = 1)\n",
        "        min_dist_difference = torch.tensor(difference[dist == dist.min()])\n",
        "        \n",
        "        u, v, e = self.encoding_func(A, b, c, x_assignment)\n",
        "\n",
        "        return u, v, e, min_dist_difference"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEx5X_A8ugzV"
      },
      "source": [
        "#### Totally random assignment function\n",
        "\n",
        "But first draw uniformly the number of 1-valued variables, then the number of variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9UAuK_DvMwk"
      },
      "source": [
        "def random_assignment(A, b, c):\n",
        "    m, n = A.shape\n",
        "    num_ones = np.random.randint(n + 1)\n",
        "    assignment = np.zeros_like(c, dtype = bool)\n",
        "    assignment[np.random.choice(n, size = num_ones, replace = False)] = True\n",
        "    return assignment"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dense_stack(*args, output_relu = True):\n",
        "    seq = nn.Sequential()\n",
        "    for i in range(1, len(args)):\n",
        "        seq.add_module(f'dense {i-1}', nn.Linear(args[i-1], args[i]))\n",
        "        if i < len(args) - 1 or output_relu:\n",
        "            seq.add_module(f'relu {i-1}', nn.ReLU())\n",
        "    return seq\n",
        "\n",
        "class HalfConvolution(nn.Module):\n",
        "    '''\n",
        "    input: a bipartite graph\n",
        "    u: features of the nodes on the same side (U, F) \n",
        "    v: features of the nodes on the opposite side (V, G)\n",
        "    e: featuers of of the edges (V, U, H)\n",
        "\n",
        "    g_args (F+G+H, ..., D)\n",
        "    f_args (F+D, ...)\n",
        "    '''\n",
        "    def __init__(self, *, f_args, g_args):\n",
        "        super().__init__()\n",
        "        self.g = dense_stack(*g_args)\n",
        "        self.f = dense_stack(*f_args)\n",
        "    def forward(self, u, v, e):\n",
        "        U, F = u.shape\n",
        "        V, G = v.shape\n",
        "        _, _, H = e.shape\n",
        "        assert e.shape == (V, U, H)\n",
        "        \n",
        "        g_out, _ = self.g(torch.cat([u.unsqueeze(-3).expand(V, U, F), v.unsqueeze(-2).expand(V, U, G), e], axis = -1)).max(axis = -3) # (V, U, D) to (U, D)\n",
        "        out = self.f(torch.cat([u, g_out], axis = -1))\n",
        "        return out\n",
        "\n",
        "class HalfConvolutionModel(nn.Module):\n",
        "    '''\n",
        "    input: same as HalfConvolution\n",
        "\n",
        "    variable_args \n",
        "    constraits_args\n",
        "    final_args\n",
        "    '''\n",
        "    def __init__(self, *, v_feats: int, c_feats: int, e_feats: list, g_hidden_neurons: list, f_hidden_neurons: list, out_neurons: list):\n",
        "        super().__init__()\n",
        "        self.half_conv = HalfConvolution(\n",
        "            g_args = [v_feats+c_feats+e_feats] + g_hidden_neurons,\n",
        "            f_args = [v_feats + g_hidden_neurons[-1]] + f_hidden_neurons\n",
        "        )\n",
        "        self.out = dense_stack(f_hidden_neurons[-1], *out_neurons, output_relu = False)\n",
        "    \n",
        "    def forward(self, u, v, e):\n",
        "        out = self.half_conv(u, v, e)  \n",
        "        return self.out(out)\n",
        "    \n",
        "    def predict(self, A, b, c, x):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "### Set up the dataset and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = HalfConvolutionModel(v_feats= 5, c_feats= 5, e_feats=1, g_hidden_neurons=[64, 64], f_hidden_neurons=[64, 64], out_neurons=[64, 1])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP2gcada-6Xj"
      },
      "source": [
        "ds = BinaryIPDataset(problem_func = problem_func, size_func = size_func, assignment_func = random_assignment, encoding_func= encoding_func)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = np.sqrt(0.1))\n",
        "Loss = torch.nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "\n",
        "num_batch, batch_size = 100, 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Academic license - for non-commercial use only - expires 2022-08-04\nUsing license file C:\\Users\\sunha\\gurobi.lic\nBatch 0, loss: 0.694\nBatch 1, loss: 0.695\nBatch 2, loss: 0.674\nBatch 3, loss: 0.668\nBatch 4, loss: 0.603\nBatch 5, loss: 0.566\nBatch 6, loss: 0.473\nBatch 7, loss: 0.363\nBatch 8, loss: 0.282\nBatch 9, loss: 0.252\nBatch 10, loss: 0.224\nBatch 11, loss: 0.243\nBatch 12, loss: 0.220\nBatch 13, loss: 0.223\nBatch 14, loss: 0.190\nBatch 15, loss: 0.155\nBatch 16, loss: 0.128\nBatch 17, loss: 0.121\nBatch 18, loss: 0.157\nBatch 19, loss: 0.139\nBatch 20, loss: 0.112\nBatch 21, loss: 0.119\nBatch 22, loss: 0.103\nBatch 23, loss: 0.093\nBatch 24, loss: 0.090\nBatch 25, loss: 0.077\nBatch 26, loss: 0.084\nBatch 27, loss: 0.081\nBatch 28, loss: 0.070\nBatch 29, loss: 0.064\nBatch 30, loss: 0.074\nBatch 31, loss: 0.073\nBatch 32, loss: 0.062\nBatch 33, loss: 0.067\nBatch 34, loss: 0.062\nBatch 35, loss: 0.058\nBatch 36, loss: 0.059\nBatch 37, loss: 0.056\nBatch 38, loss: 0.083\nBatch 39, loss: 0.076\nBatch 40, loss: 0.065\nBatch 41, loss: 0.072\nBatch 42, loss: 0.056\nBatch 43, loss: 0.056\nBatch 44, loss: 0.081\nBatch 45, loss: 0.046\nBatch 46, loss: 0.057\nBatch 47, loss: 0.065\nBatch 48, loss: 0.069\nBatch 49, loss: 0.060\nBatch 50, loss: 0.051\nBatch 51, loss: 0.061\nBatch 52, loss: 0.066\nBatch 53, loss: 0.063\nBatch 54, loss: 0.062\nBatch 55, loss: 0.050\nBatch 56, loss: 0.054\nBatch 57, loss: 0.058\nBatch 58, loss: 0.053\nBatch 59, loss: 0.058\nBatch 60, loss: 0.054\nBatch 61, loss: 0.073\nBatch 62, loss: 0.071\nBatch 63, loss: 0.040\nBatch 64, loss: 0.055\nBatch 65, loss: 0.058\nBatch 66, loss: 0.062\nBatch 67, loss: 0.058\nBatch 68, loss: 0.059\nBatch 69, loss: 0.052\nBatch 70, loss: 0.054\nBatch 71, loss: 0.054\nBatch 72, loss: 0.072\nBatch 73, loss: 0.043\nBatch 74, loss: 0.057\nBatch 75, loss: 0.095\nBatch 76, loss: 0.052\nBatch 77, loss: 0.072\nBatch 78, loss: 0.057\nBatch 79, loss: 0.071\nBatch 80, loss: 0.053\nBatch 81, loss: 0.052\nBatch 82, loss: 0.055\nBatch 83, loss: 0.066\nBatch 84, loss: 0.061\nBatch 85, loss: 0.046\nBatch 86, loss: 0.072\nBatch 87, loss: 0.071\nBatch 88, loss: 0.052\nBatch 89, loss: 0.069\nBatch 90, loss: 0.039\nBatch 91, loss: 0.058\nBatch 92, loss: 0.069\nBatch 93, loss: 0.069\nBatch 94, loss: 0.037\nBatch 95, loss: 0.060\nBatch 96, loss: 0.060\nBatch 97, loss: 0.048\nBatch 98, loss: 0.060\nBatch 99, loss: 0.054\n"
        }
      ],
      "source": [
        "for i in range(num_batch):\n",
        "    optimizer.zero_grad()\n",
        "    loss = torch.tensor(0.0)\n",
        "    for j in range(batch_size):\n",
        "        u, v, e, min_dist_diff = ds[j]\n",
        "        N_sols, _ = min_dist_diff.shape\n",
        "\n",
        "        scores = model(u, v, e).t().expand(N_sols, -1) # (n, 1) -> (1, n) -> (N_sols, n)\n",
        "\n",
        "        _loss = Loss(input = scores, target = min_dist_diff.float()).mean(axis = 1)\n",
        "        loss = loss + _loss.min() / batch_size \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    print(f'Batch {i}, loss: {loss.item():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "tree-search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python38864bitpytorchconda7790f7e226db4d799e23e7669a0b2db2",
      "display_name": "Python 3.8.8 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}