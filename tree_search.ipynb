{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xAH0nvLoNTn"
      },
      "source": [
        "# Tree search with 0-1 integer programming\n",
        "\n",
        "This notebook follows from the meeting on 5 Oct, 2021. I think it's a good time to expand the action space and allow random flipping of variables. Before I get make myself crazy again, I want to make sure that it achieves something.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7GfgJYHtli-"
      },
      "source": [
        "I wish one day they can automatically included these for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zhCTOQJGtod8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import gurobipy as gp \n",
        "from gurobipy import GRB\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3bFLlf3qNoT"
      },
      "source": [
        "## Gurobi to actually solve the 0-1 integer programs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O-yWlW76qvH3"
      },
      "outputs": [],
      "source": [
        "def gurobi_optimal(A, b, c, *, num_solution_limit = 32):\n",
        "    m, n = A.shape\n",
        "    assert b.shape == (m, ) and c.shape == (n, )\n",
        "\n",
        "    ip = gp.Model()\n",
        "    x_var = ip.addMVar(n, lb = 0.0, ub = float('inf'), vtype = GRB.BINARY, name = 'x')\n",
        "    ip.addConstr(A @ x_var <= b)\n",
        "    ip.setObjective(c @ x_var, GRB.MAXIMIZE)\n",
        "    ip.setParam('OutputFlag', 0)\n",
        "    ip.setParam('PoolSearchMode', 2)\n",
        "    ip.setParam('PoolSolutions', num_solution_limit)\n",
        "    ip.optimize()\n",
        "\n",
        "    solutions = []\n",
        "    obj = c @ np.round(x_var.x)\n",
        "    for i in range(ip.SolCount):\n",
        "        ip.setParam('SolutionNumber', i)\n",
        "        sol = np.array(ip.xn).round().astype(bool)\n",
        "        if c @ sol < obj:\n",
        "            break\n",
        "        else:\n",
        "            solutions.append(sol)\n",
        "    return obj, np.stack(solutions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKThhWpJsc7X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ncCnLY4sdVI"
      },
      "source": [
        "## Data and dataset\n",
        "### Problems to be explored\n",
        "Please refer to the Dataset part for details.\n",
        "\n",
        "#### Set Cover Problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KPIeIR4bsLgs"
      },
      "outputs": [],
      "source": [
        "from problems.setcover import *\n",
        "problem_func = generate_setcover\n",
        "encoding_func = setcover_encoding\n",
        "def size_func():\n",
        "    return {'nrows': 100, 'ncols': 200, 'density': np.random.uniform(0.1, 0.25)}\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0rlaO539Gu_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhS8_QTAsmmr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6vQyn6PvVFN"
      },
      "source": [
        "#### Verify the problem size\n",
        "- $A\\in \\mathbb{R}^{m\\times n}$\n",
        "- $b\\in \\mathbb{R}^m$\n",
        "- $c\\in \\mathbb{R}^n$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NwmeD98pvUH9"
      },
      "outputs": [],
      "source": [
        "def valid_dim(A, b, c, x = None):\n",
        "    m, n = A.shape \n",
        "    return b.shape == (m, ) and c.shape == (n,) and (x is None or x.shape == (n, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6UlDt8rsm6L"
      },
      "source": [
        "### Encoding of the problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xL0XgfospHp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo4h6mANsp0h"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XsEyGpKetWtw"
      },
      "outputs": [],
      "source": [
        "class BinaryIPDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Generate a problem instance with a random, non-terminal assignment\n",
        "    '''\n",
        "    def __init__(self, *, problem_func, size_func, assignment_func, encoding_func):\n",
        "        '''\n",
        "        problem_func: problem to be explored\n",
        "        size_func: size as a input parameter to the problem_func\n",
        "        problem_func(**size_func()) should work\n",
        "        \n",
        "        assignment_func: used to generate an assignemnt for the given problem\n",
        "        encoding_func: \n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.problem_func = problem_func\n",
        "        self.size_func = size_func\n",
        "        self.assignment_func = assignment_func\n",
        "        self.encoding_func = encoding_func\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError # not needed\n",
        "\n",
        "    def __getitem__(self, index:int):\n",
        "        # get a random instance in the class of problem with a random size\n",
        "        A, b, c = self.problem_func(**self.size_func())\n",
        "\n",
        "        # get a meaningful but random variable assignemnt\n",
        "        x_assignment = self.assignment_func(A, b, c)\n",
        "        \n",
        "        # get a random intermediate solution\n",
        "        _, solutions = gurobi_optimal(A, b, c)\n",
        "\n",
        "        difference = np.logical_xor(x_assignment, solutions)\n",
        "        dist = difference.sum(axis = 1)\n",
        "        min_dist_difference = torch.tensor(difference[dist == dist.min()])\n",
        "        \n",
        "        u, v, e = self.encoding_func(A, b, c, x_assignment)\n",
        "\n",
        "        return u, v, e, min_dist_difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEx5X_A8ugzV"
      },
      "source": [
        "#### Totally random assignment function\n",
        "\n",
        "But first draw uniformly the number of 1-valued variables, then the number of variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E9UAuK_DvMwk"
      },
      "outputs": [],
      "source": [
        "def random_assignment(A, b, c):\n",
        "    m, n = A.shape\n",
        "    num_ones = np.random.randint(n + 1)\n",
        "    assignment = np.zeros_like(c, dtype = bool)\n",
        "    assignment[np.random.choice(n, size = num_ones, replace = False)] = True\n",
        "    return assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dense_stack(*args, output_relu = True):\n",
        "    seq = nn.Sequential()\n",
        "    for i in range(1, len(args)):\n",
        "        seq.add_module(f'dense {i-1}', nn.Linear(args[i-1], args[i]))\n",
        "        if i < len(args) - 1 or output_relu:\n",
        "            seq.add_module(f'relu {i-1}', nn.ReLU())\n",
        "    return seq\n",
        "\n",
        "class HalfConvolution(nn.Module):\n",
        "    '''\n",
        "    input: a bipartite graph\n",
        "    u: features of the nodes on the same side (U, F) \n",
        "    v: features of the nodes on the opposite side (V, G)\n",
        "    e: featuers of of the edges (V, U, H)\n",
        "\n",
        "    g_args (F+G+H, ..., D)\n",
        "    f_args (F+D, ...)\n",
        "    '''\n",
        "    def __init__(self, *, f_args, g_args):\n",
        "        super().__init__()\n",
        "        self.g = dense_stack(*g_args)\n",
        "        self.f = dense_stack(*f_args)\n",
        "    def forward(self, u, v, e):\n",
        "        U, F = u.shape\n",
        "        V, G = v.shape\n",
        "        _, _, H = e.shape\n",
        "        assert e.shape == (V, U, H)\n",
        "        \n",
        "        g_out, _ = self.g(torch.cat([u.unsqueeze(-3).expand(V, U, F), v.unsqueeze(-2).expand(V, U, G), e], axis = -1)).max(axis = -3) # (V, U, D) to (U, D)\n",
        "        out = self.f(torch.cat([u, g_out], axis = -1))\n",
        "        return out\n",
        "\n",
        "class HalfConvolutionModel(nn.Module):\n",
        "    '''\n",
        "    input: same as HalfConvolution\n",
        "\n",
        "    variable_args \n",
        "    constraits_args\n",
        "    final_args\n",
        "    '''\n",
        "    def __init__(self, *, v_feats: int, c_feats: int, e_feats: list, g_hidden_neurons: list, f_hidden_neurons: list, out_neurons: list):\n",
        "        super().__init__()\n",
        "        self.half_conv = HalfConvolution(\n",
        "            g_args = [v_feats+c_feats+e_feats] + g_hidden_neurons,\n",
        "            f_args = [v_feats + g_hidden_neurons[-1]] + f_hidden_neurons\n",
        "        )\n",
        "        self.out = dense_stack(f_hidden_neurons[-1], *out_neurons, output_relu = False)\n",
        "    \n",
        "    def forward(self, u, v, e):\n",
        "        out = self.half_conv(u, v, e)  \n",
        "        return self.out(out)\n",
        "    \n",
        "    def predict(self, A, b, c, x):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "### Set up the dataset and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = HalfConvolutionModel(v_feats= 5, c_feats= 5, e_feats=1, g_hidden_neurons=[64, 64], f_hidden_neurons=[64, 64], out_neurons=[64, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eP2gcada-6Xj"
      },
      "outputs": [],
      "source": [
        "ds = BinaryIPDataset(problem_func = problem_func, size_func = size_func, assignment_func = random_assignment, encoding_func= encoding_func)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = np.sqrt(0.1))\n",
        "Loss = torch.nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "\n",
        "num_batch, batch_size = 100, 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Academic license - for non-commercial use only - expires 2022-08-04\nUsing license file C:\\Users\\sunha\\gurobi.lic\nBatch 0, loss: 0.694\nBatch 1, loss: 0.695\nBatch 2, loss: 0.674\nBatch 3, loss: 0.668\nBatch 4, loss: 0.603\nBatch 5, loss: 0.566\nBatch 6, loss: 0.473\nBatch 7, loss: 0.363\nBatch 8, loss: 0.282\nBatch 9, loss: 0.252\nBatch 10, loss: 0.224\nBatch 11, loss: 0.243\nBatch 12, loss: 0.220\nBatch 13, loss: 0.223\nBatch 14, loss: 0.190\nBatch 15, loss: 0.155\nBatch 16, loss: 0.128\nBatch 17, loss: 0.121\nBatch 18, loss: 0.157\nBatch 19, loss: 0.139\nBatch 20, loss: 0.112\nBatch 21, loss: 0.119\nBatch 22, loss: 0.103\nBatch 23, loss: 0.093\nBatch 24, loss: 0.090\nBatch 25, loss: 0.077\nBatch 26, loss: 0.084\nBatch 27, loss: 0.081\nBatch 28, loss: 0.070\nBatch 29, loss: 0.064\nBatch 30, loss: 0.074\nBatch 31, loss: 0.073\nBatch 32, loss: 0.062\nBatch 33, loss: 0.067\nBatch 34, loss: 0.062\nBatch 35, loss: 0.058\nBatch 36, loss: 0.059\nBatch 37, loss: 0.056\nBatch 38, loss: 0.083\nBatch 39, loss: 0.076\nBatch 40, loss: 0.065\nBatch 41, loss: 0.072\nBatch 42, loss: 0.056\nBatch 43, loss: 0.056\nBatch 44, loss: 0.081\nBatch 45, loss: 0.046\nBatch 46, loss: 0.057\nBatch 47, loss: 0.065\nBatch 48, loss: 0.069\nBatch 49, loss: 0.060\nBatch 50, loss: 0.051\nBatch 51, loss: 0.061\nBatch 52, loss: 0.066\nBatch 53, loss: 0.063\nBatch 54, loss: 0.062\nBatch 55, loss: 0.050\nBatch 56, loss: 0.054\nBatch 57, loss: 0.058\nBatch 58, loss: 0.053\nBatch 59, loss: 0.058\nBatch 60, loss: 0.054\nBatch 61, loss: 0.073\nBatch 62, loss: 0.071\nBatch 63, loss: 0.040\nBatch 64, loss: 0.055\nBatch 65, loss: 0.058\nBatch 66, loss: 0.062\nBatch 67, loss: 0.058\nBatch 68, loss: 0.059\nBatch 69, loss: 0.052\nBatch 70, loss: 0.054\nBatch 71, loss: 0.054\nBatch 72, loss: 0.072\nBatch 73, loss: 0.043\nBatch 74, loss: 0.057\nBatch 75, loss: 0.095\nBatch 76, loss: 0.052\nBatch 77, loss: 0.072\nBatch 78, loss: 0.057\nBatch 79, loss: 0.071\nBatch 80, loss: 0.053\nBatch 81, loss: 0.052\nBatch 82, loss: 0.055\nBatch 83, loss: 0.066\nBatch 84, loss: 0.061\nBatch 85, loss: 0.046\nBatch 86, loss: 0.072\nBatch 87, loss: 0.071\nBatch 88, loss: 0.052\nBatch 89, loss: 0.069\nBatch 90, loss: 0.039\nBatch 91, loss: 0.058\nBatch 92, loss: 0.069\nBatch 93, loss: 0.069\nBatch 94, loss: 0.037\nBatch 95, loss: 0.060\nBatch 96, loss: 0.060\nBatch 97, loss: 0.048\nBatch 98, loss: 0.060\nBatch 99, loss: 0.054\n"
        }
      ],
      "source": [
        "for i in range(num_batch):\n",
        "    optimizer.zero_grad()\n",
        "    loss = torch.tensor(0.0)\n",
        "    for j in range(batch_size):\n",
        "        u, v, e, min_dist_diff = ds[j]\n",
        "        N_sols, _ = min_dist_diff.shape\n",
        "\n",
        "        scores = model(u, v, e).t().expand(N_sols, -1) # (n, 1) -> (1, n) -> (N_sols, n)\n",
        "\n",
        "        _loss = Loss(input = scores, target = min_dist_diff.float()).mean(axis = 1)\n",
        "        loss = loss + _loss.min() / batch_size \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    print(f'Batch {i}, loss: {loss.item():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model, './model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.load('./model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test set generating\n",
        "def generate_test(len_test=1026, problem_func=problem_func):\n",
        "    problems, objectives = [], []\n",
        "    for i in range(len_test):\n",
        "        A, b, c = problem_func(**size_func())\n",
        "        problems.append((A, b, c))\n",
        "        obj, _ = gurobi_optimal(A, b, c)\n",
        "        objectives.append(obj)\n",
        "    \n",
        "    return problems, objectives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "p, o = generate_test(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimator(A, b, c, x, encoding_func=encoding_func):\n",
        "    if model.training:\n",
        "        model.eval()\n",
        "    with torch.no_grad():\n",
        "        u, v, e = encoding_func(A, b, c, x)\n",
        "        scores = model(u, v, e).detach().numpy().flatten()\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\n",
        "def tree_search(A, b, c, objective, estimator, threshold=0.5, max_children=2, num_step_limit = np.inf):\n",
        "    \"\"\"\n",
        "    ## Parameter\n",
        "\n",
        "    `A, b, c, objective`: problem\n",
        "\n",
        "    `estimator`: score function\n",
        "    \n",
        "    `threshold`: which score to choose\n",
        "    \n",
        "    `max_children`: children of each node\n",
        "\n",
        "    `num_step_limit`: max times to run\n",
        "    \"\"\"\n",
        "    m, n = A.shape\n",
        "    assert b.shape == (m, ) and c.shape == (n, )\n",
        "    tree = collections.deque()\n",
        "\n",
        "    step_count = 0\n",
        "    run = True\n",
        "    best_x = np.zeros_like(c, dtype = bool)\n",
        "    current_obj = 0\n",
        "    while step_count < num_step_limit and run:  \n",
        "        if tree:\n",
        "            x = tree.popleft()\n",
        "        else:\n",
        "            x = np.zeros_like(c, dtype = bool)\n",
        "\n",
        "        score = estimator(A, b, c, x)\n",
        "        score[score < threshold] = 0\n",
        "        indices = np.argsort(score, kind='quicksort')[-1:-(max_children+1):-1]\n",
        "        for index in indices:\n",
        "            if score[index] == 0:\n",
        "                break\n",
        "            else: \n",
        "                x[index] = ~x[index]\n",
        "                valid = (A @ x <= b).all()\n",
        "                if valid:\n",
        "                    tree.append(x)\n",
        "                    # When to stop?\n",
        "                    if c @ x >= current_obj:\n",
        "                        best_x = x.copy()\n",
        "                        current_obj = c @ x\n",
        "                    if c @ x == objective:\n",
        "                        run = False\n",
        "                        best_x = x.copy()\n",
        "        \n",
        "        if not tree and step_count != 0:\n",
        "            run = False\n",
        "\n",
        "        step_count += 1\n",
        "        # print(\"Current tree size: {}\".format(len(tree)))\n",
        "\n",
        "    return best_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Objective 1723, Result 1462\n",
            "[2. 0. 1. 3. 0. 3. 2. 1. 2. 1. 2. 3. 5. 2. 0. 1. 3. 2. 0. 0.]\n",
            "Objective 2001, Result 1966\n",
            "[0. 1. 2. 2. 0. 0. 0. 0. 1. 2. 1. 1. 1. 0. 0. 1. 1. 0. 3. 0.]\n",
            "Objective 1791, Result 1375\n",
            "[1. 6. 0. 1. 5. 2. 2. 1. 1. 1. 7. 3. 1. 2. 1. 1. 2. 3. 2. 2.]\n",
            "Objective 1791, Result 1781\n",
            "[1. 0. 1. 2. 0. 0. 1. 0. 0. 0. 0. 2. 0. 2. 1. 1. 0. 0. 0. 0.]\n",
            "Objective 1651, Result 1612\n",
            "[0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 2. 2. 1. 1. 2. 0. 1. 1.]\n",
            "Objective 1873, Result 1709\n",
            "[0. 1. 1. 2. 0. 0. 2. 1. 2. 3. 1. 2. 0. 1. 1. 1. 2. 2. 1. 2.]\n",
            "Objective 1682, Result 1644\n",
            "[0. 0. 1. 0. 1. 2. 2. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 1. 2. 1.]\n",
            "Objective 1651, Result 1551\n",
            "[1. 2. 0. 1. 0. 0. 0. 0. 3. 0. 2. 1. 0. 0. 0. 2. 1. 0. 0. 2.]\n",
            "Objective 2095, Result 1971\n",
            "[1. 1. 1. 0. 2. 1. 3. 0. 0. 0. 1. 1. 3. 3. 0. 0. 0. 2. 1. 2.]\n",
            "Objective 1948, Result 1880\n",
            "[1. 0. 0. 1. 2. 2. 2. 3. 3. 1. 1. 0. 4. 0. 2. 3. 0. 1. 0. 2.]\n",
            "Objective 1745, Result 1729\n",
            "[1. 0. 1. 0. 1. 1. 2. 2. 1. 0. 0. 1. 0. 0. 1. 0. 3. 0. 2. 0.]\n",
            "Objective 1866, Result 1731\n",
            "[3. 1. 0. 2. 1. 2. 2. 0. 1. 2. 3. 1. 3. 2. 1. 2. 1. 1. 0. 1.]\n",
            "Objective 1893, Result 1887\n",
            "[0. 0. 1. 1. 0. 0. 1. 0. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Objective 1609, Result 1313\n",
            "[1. 2. 1. 3. 3. 2. 0. 0. 2. 5. 0. 4. 2. 3. 1. 0. 1. 3. 2. 2.]\n",
            "Objective 1956, Result 1865\n",
            "[0. 2. 1. 1. 1. 1. 1. 0. 0. 2. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
            "Objective 1843, Result 1399\n",
            "[2. 0. 2. 1. 4. 1. 1. 2. 2. 4. 2. 3. 2. 3. 3. 2. 3. 4. 2. 3.]\n",
            "Objective 1864, Result 1821\n",
            "[1. 2. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0. 2. 2. 0. 0. 1. 0. 0.]\n",
            "Objective 1687, Result 1598\n",
            "[1. 1. 0. 0. 0. 1. 0. 2. 0. 1. 1. 1. 0. 1. 0. 2. 1. 0. 1. 0.]\n",
            "Objective 1908, Result 1871\n",
            "[0. 2. 1. 1. 2. 0. 3. 1. 0. 0. 3. 0. 0. 0. 1. 0. 2. 0. 2. 0.]\n",
            "Objective 1568, Result 1537\n",
            "[2. 0. 2. 1. 1. 1. 0. 1. 3. 1. 1. 0. 2. 0. 3. 4. 0. 0. 2. 0.]\n",
            "Objective 2119, Result 2102\n",
            "[0. 1. 2. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 2. 2. 1.]\n",
            "Objective 1865, Result 1490\n",
            "[3. 1. 2. 3. 3. 1. 1. 0. 0. 2. 2. 0. 2. 4. 2. 0. 1. 5. 0. 0.]\n",
            "Objective 2039, Result 2005\n",
            "[2. 1. 4. 0. 1. 1. 2. 0. 1. 2. 2. 0. 2. 1. 1. 0. 1. 2. 0. 2.]\n",
            "Objective 1831, Result 1790\n",
            "[2. 3. 1. 1. 2. 1. 0. 0. 0. 0. 0. 0. 1. 2. 1. 1. 3. 0. 1. 0.]\n",
            "Objective 2022, Result 1640\n",
            "[2. 2. 0. 5. 2. 1. 2. 1. 3. 1. 3. 1. 3. 4. 2. 2. 3. 1. 5. 1.]\n",
            "Objective 1759, Result 1725\n",
            "[1. 1. 2. 3. 1. 0. 1. 2. 2. 2. 1. 0. 0. 1. 2. 1. 0. 1. 2. 1.]\n",
            "Objective 2006, Result 1861\n",
            "[1. 1. 1. 2. 1. 2. 0. 1. 2. 2. 0. 1. 1. 0. 1. 0. 3. 1. 1. 1.]\n",
            "Objective 1944, Result 1567\n",
            "[2. 2. 6. 2. 1. 1. 3. 1. 1. 4. 3. 4. 0. 3. 4. 2. 3. 0. 2. 2.]\n",
            "Objective 1799, Result 1658\n",
            "[3. 1. 1. 2. 0. 0. 3. 2. 0. 0. 2. 2. 3. 3. 0. 0. 2. 4. 4. 0.]\n",
            "Objective 1641, Result 1619\n",
            "[3. 0. 1. 0. 1. 3. 1. 2. 0. 1. 1. 0. 0. 1. 2. 1. 1. 0. 1. 3.]\n",
            "Objective 1652, Result 1595\n",
            "[2. 2. 0. 1. 0. 2. 0. 0. 2. 0. 2. 2. 2. 0. 1. 3. 2. 2. 0. 0.]\n",
            "Objective 1641, Result 1598\n",
            "[2. 1. 3. 2. 1. 0. 1. 0. 0. 0. 0. 0. 1. 2. 2. 1. 1. 2. 1. 2.]\n",
            "Objective 1590, Result 1316\n",
            "[0. 2. 3. 1. 2. 1. 1. 0. 1. 4. 1. 1. 5. 0. 2. 2. 0. 0. 3. 0.]\n",
            "Objective 1860, Result 1817\n",
            "[3. 3. 2. 0. 0. 0. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 0. 4. 2.]\n",
            "Objective 1833, Result 1832\n",
            "[0. 1. 3. 0. 0. 0. 1. 2. 0. 0. 1. 0. 1. 0. 1. 0. 2. 0. 0. 2.]\n",
            "Objective 1972, Result 1962\n",
            "[1. 0. 1. 1. 1. 1. 1. 4. 1. 0. 0. 0. 0. 2. 0. 0. 3. 0. 0. 2.]\n",
            "Objective 1954, Result 1719\n",
            "[1. 0. 0. 0. 4. 0. 2. 1. 4. 1. 1. 0. 0. 0. 5. 1. 2. 0. 2. 0.]\n",
            "Objective 1651, Result 1651\n",
            "[1. 3. 0. 0. 1. 0. 2. 0. 2. 0. 1. 0. 0. 0. 0. 0. 3. 0. 1. 0.]\n",
            "Objective 1862, Result 1834\n",
            "[1. 0. 2. 0. 1. 3. 1. 3. 1. 1. 0. 0. 1. 0. 1. 1. 1. 2. 1. 2.]\n",
            "Objective 2004, Result 1961\n",
            "[0. 1. 0. 0. 0. 1. 0. 0. 2. 0. 1. 1. 0. 0. 0. 1. 3. 3. 0. 2.]\n",
            "Objective 2237, Result 2102\n",
            "[2. 4. 0. 2. 2. 1. 1. 0. 0. 1. 1. 3. 2. 1. 1. 2. 2. 2. 5. 0.]\n",
            "Objective 1811, Result 1664\n",
            "[2. 1. 0. 4. 5. 1. 3. 3. 0. 5. 3. 2. 0. 0. 4. 3. 3. 2. 0. 6.]\n",
            "Objective 2134, Result 2050\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 2. 0. 1. 0. 1. 0. 1. 0. 0.]\n",
            "Objective 2104, Result 2085\n",
            "[0. 2. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 3. 1. 0. 1. 1. 0. 1. 1.]\n",
            "Objective 1871, Result 1774\n",
            "[1. 2. 4. 1. 1. 1. 3. 1. 1. 0. 4. 4. 1. 0. 3. 1. 1. 4. 2. 2.]\n",
            "Objective 2074, Result 2017\n",
            "[0. 0. 2. 0. 1. 1. 4. 0. 2. 1. 1. 1. 1. 3. 2. 2. 0. 1. 4. 2.]\n",
            "Objective 1526, Result 1245\n",
            "[1. 3. 0. 2. 0. 2. 4. 0. 1. 3. 0. 2. 2. 0. 0. 2. 4. 2. 2. 1.]\n",
            "Objective 1941, Result 1928\n",
            "[2. 1. 1. 2. 0. 0. 0. 0. 2. 3. 1. 3. 1. 2. 1. 0. 2. 0. 0. 2.]\n",
            "Objective 2044, Result 2019\n",
            "[1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 2. 2. 0. 0. 2. 2.]\n",
            "Objective 1829, Result 1724\n",
            "[0. 0. 1. 1. 1. 0. 0. 1. 2. 1. 0. 1. 0. 0. 2. 1. 3. 0. 1. 0.]\n",
            "Objective 1834, Result 1753\n",
            "[1. 0. 4. 0. 2. 1. 2. 1. 1. 4. 0. 0. 1. 3. 1. 2. 1. 1. 1. 0.]\n",
            "Objective 1723, Result 1723\n",
            "[0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Objective 2057, Result 2028\n",
            "[0. 0. 2. 2. 1. 0. 1. 0. 1. 1. 2. 2. 2. 2. 2. 0. 0. 1. 0. 2.]\n",
            "Objective 1663, Result 1642\n",
            "[0. 2. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
            "Objective 2112, Result 2104\n",
            "[1. 3. 2. 1. 0. 2. 0. 3. 0. 0. 1. 1. 0. 0. 1. 2. 0. 1. 0. 1.]\n",
            "Objective 1845, Result 1475\n",
            "[2. 0. 1. 1. 2. 1. 3. 3. 2. 3. 2. 1. 0. 3. 3. 0. 1. 1. 2. 2.]\n",
            "Objective 2042, Result 1975\n",
            "[0. 1. 1. 0. 1. 1. 2. 1. 0. 1. 1. 3. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Objective 2146, Result 2016\n",
            "[4. 2. 4. 1. 0. 3. 1. 2. 0. 5. 2. 1. 0. 2. 2. 4. 1. 1. 2. 4.]\n",
            "Objective 1679, Result 1662\n",
            "[0. 0. 2. 1. 1. 0. 2. 0. 2. 2. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n",
            "Objective 1754, Result 1510\n",
            "[1. 0. 1. 2. 0. 1. 1. 0. 2. 0. 2. 1. 1. 0. 2. 0. 1. 3. 0. 2.]\n",
            "Objective 1889, Result 1845\n",
            "[5. 1. 0. 0. 1. 1. 1. 3. 0. 2. 3. 3. 2. 3. 0. 0. 5. 0. 2. 1.]\n",
            "Objective 1864, Result 1853\n",
            "[2. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0.]\n",
            "Objective 1800, Result 1758\n",
            "[0. 0. 1. 2. 0. 1. 3. 2. 0. 2. 3. 0. 3. 1. 3. 0. 3. 4. 0. 2.]\n",
            "Objective 2076, Result 1740\n",
            "[0. 1. 0. 1. 0. 1. 2. 0. 0. 1. 2. 1. 3. 4. 0. 1. 1. 2. 2. 1.]\n",
            "Objective 1725, Result 1706\n",
            "[1. 1. 2. 1. 0. 2. 0. 0. 2. 2. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.]\n",
            "Objective 1865, Result 1845\n",
            "[0. 2. 0. 1. 0. 0. 3. 1. 1. 1. 0. 0. 2. 0. 0. 1. 0. 1. 2. 0.]\n",
            "Objective 1574, Result 1462\n",
            "[0. 4. 1. 0. 0. 0. 1. 0. 0. 0. 2. 4. 1. 0. 0. 0. 1. 1. 2. 1.]\n",
            "Objective 1934, Result 1890\n",
            "[0. 0. 3. 0. 1. 2. 2. 1. 0. 0. 2. 2. 2. 1. 0. 0. 2. 0. 0. 1.]\n",
            "Objective 2060, Result 2030\n",
            "[0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 2. 2. 0.]\n",
            "Objective 1972, Result 1934\n",
            "[1. 1. 0. 1. 1. 2. 0. 1. 1. 2. 0. 3. 0. 1. 2. 0. 0. 3. 1. 4.]\n",
            "Objective 1983, Result 1902\n",
            "[0. 3. 0. 1. 0. 0. 0. 3. 1. 3. 1. 3. 2. 3. 0. 1. 0. 1. 0. 1.]\n",
            "Objective 2064, Result 1958\n",
            "[0. 3. 2. 1. 0. 1. 1. 0. 5. 3. 1. 0. 0. 2. 1. 0. 3. 3. 2. 1.]\n",
            "Objective 1975, Result 1955\n",
            "[2. 2. 0. 0. 2. 1. 0. 3. 1. 2. 0. 1. 2. 1. 0. 0. 0. 2. 0. 2.]\n",
            "Objective 1922, Result 1870\n",
            "[3. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 2. 2. 1. 0. 0. 1. 0.]\n",
            "Objective 1796, Result 1162\n",
            "[0. 5. 3. 2. 1. 2. 3. 2. 3. 4. 2. 1. 5. 3. 2. 1. 3. 3. 2. 2.]\n",
            "Objective 1822, Result 1764\n",
            "[1. 0. 1. 1. 1. 1. 1. 2. 0. 0. 0. 3. 0. 0. 0. 2. 1. 0. 0. 0.]\n",
            "Objective 1786, Result 1758\n",
            "[3. 0. 1. 1. 0. 1. 0. 2. 1. 2. 2. 2. 0. 0. 0. 2. 0. 2. 1. 0.]\n",
            "Objective 2082, Result 2060\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 2. 1. 0. 2. 0.]\n",
            "Objective 1661, Result 1497\n",
            "[3. 1. 1. 0. 1. 1. 2. 0. 1. 0. 0. 1. 0. 1. 0. 0. 2. 1. 0. 1.]\n",
            "Objective 1991, Result 1678\n",
            "[6. 3. 2. 4. 5. 1. 3. 3. 6. 3. 2. 4. 4. 0. 0. 2. 5. 1. 1. 4.]\n",
            "Objective 1990, Result 1903\n",
            "[1. 0. 3. 1. 0. 2. 1. 0. 2. 0. 3. 3. 3. 1. 0. 1. 1. 3. 0. 3.]\n",
            "Objective 1899, Result 1843\n",
            "[1. 0. 2. 2. 1. 2. 1. 2. 3. 1. 1. 0. 4. 1. 1. 2. 2. 2. 2. 0.]\n",
            "Objective 1738, Result 1700\n",
            "[2. 3. 3. 2. 0. 0. 0. 0. 0. 1. 2. 2. 0. 0. 1. 1. 0. 0. 1. 1.]\n",
            "Objective 1578, Result 1321\n",
            "[1. 1. 1. 0. 0. 2. 5. 2. 1. 2. 0. 3. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
            "Objective 1990, Result 1939\n",
            "[1. 0. 1. 3. 1. 1. 1. 0. 1. 2. 1. 0. 0. 2. 0. 1. 1. 2. 0. 0.]\n",
            "Objective 1794, Result 1719\n",
            "[2. 2. 2. 2. 0. 3. 3. 1. 2. 0. 1. 2. 1. 2. 1. 1. 3. 3. 2. 3.]\n",
            "Objective 1893, Result 1858\n",
            "[0. 0. 0. 2. 1. 0. 2. 1. 1. 0. 0. 3. 0. 4. 1. 0. 4. 1. 0. 4.]\n",
            "Objective 2048, Result 1808\n",
            "[0. 1. 1. 2. 0. 1. 1. 2. 2. 2. 2. 0. 3. 3. 2. 1. 5. 2. 0. 0.]\n",
            "Objective 1966, Result 1714\n",
            "[3. 0. 1. 1. 6. 4. 4. 3. 0. 2. 2. 3. 0. 5. 4. 2. 5. 3. 4. 2.]\n",
            "Objective 1925, Result 1886\n",
            "[0. 1. 0. 1. 0. 2. 0. 0. 2. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.]\n",
            "Objective 2061, Result 1899\n",
            "[1. 3. 1. 2. 1. 2. 1. 3. 0. 0. 1. 1. 3. 2. 1. 0. 2. 1. 3. 3.]\n",
            "Objective 2060, Result 2015\n",
            "[1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 3. 2. 1. 1. 1. 0. 3. 3. 0. 0.]\n",
            "Objective 2024, Result 2023\n",
            "[2. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 2. 2. 3. 0. 0. 0. 0. 1. 0.]\n",
            "Objective 1794, Result 1783\n",
            "[1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 2. 2. 0. 0. 2.]\n",
            "Objective 1990, Result 1969\n",
            "[0. 0. 0. 3. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
            "Objective 1522, Result 1504\n",
            "[0. 0. 1. 2. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 2. 0. 1. 1. 2. 0.]\n",
            "Objective 1830, Result 1732\n",
            "[0. 1. 0. 0. 2. 0. 0. 3. 1. 2. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Objective 2026, Result 1982\n",
            "[0. 1. 1. 1. 0. 2. 0. 3. 2. 1. 1. 0. 2. 0. 1. 0. 0. 0. 1. 0.]\n",
            "Objective 1938, Result 1679\n",
            "[2. 3. 3. 2. 6. 3. 0. 4. 7. 2. 6. 5. 3. 2. 1. 3. 3. 3. 3. 2.]\n",
            "Objective 1816, Result 1697\n",
            "[1. 0. 1. 1. 0. 0. 0. 2. 2. 0. 0. 1. 2. 0. 1. 0. 0. 1. 0. 2.]\n"
          ]
        }
      ],
      "source": [
        "for problem, objective in zip(p, o):\n",
        "    A, b, c = problem\n",
        "    best_x = tree_search(A, b, c, objective, estimator=estimator, threshold=0.0, num_step_limit=4096)\n",
        "    print(\"Objective %d, Result %d\" % (objective, c @ best_x))\n",
        "    print(b - A @ best_x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tree-search.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "dca4efdd400e53e6663797bbd1b46cf726b4dffb9a4cd6e80b88fc0d122f2781"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('pytorch_env': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
