{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xAH0nvLoNTn"
      },
      "source": [
        "# Tree search with 0-1 integer programming\n",
        "\n",
        "This notebook follows from the meeting on 5 Oct, 2021. I think it's a good time to expand the action space and allow random flipping of variables. Before I get make myself crazy again, I want to make sure that it achieves something.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7GfgJYHtli-"
      },
      "source": [
        "I wish one day they can automatically included these for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zhCTOQJGtod8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import gurobipy as gp \n",
        "from gurobipy import GRB\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3bFLlf3qNoT"
      },
      "source": [
        "## Gurobi to actually solve the 0-1 integer programs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O-yWlW76qvH3"
      },
      "outputs": [],
      "source": [
        "def gurobi_optimal(A, b, c, *, num_solution_limit = 32):\n",
        "    m, n = A.shape\n",
        "    assert b.shape == (m, ) and c.shape == (n, )\n",
        "\n",
        "    ip = gp.Model()\n",
        "    x_var = ip.addMVar(n, lb = 0.0, ub = float('inf'), vtype = GRB.BINARY, name = 'x')\n",
        "    ip.addConstr(A @ x_var <= b)\n",
        "    ip.setObjective(c @ x_var, GRB.MAXIMIZE)\n",
        "    ip.setParam('OutputFlag', 0)\n",
        "    ip.setParam('PoolSearchMode', 2)\n",
        "    ip.setParam('PoolSolutions', num_solution_limit)\n",
        "    ip.optimize()\n",
        "\n",
        "    solutions = []\n",
        "    obj = c @ np.round(x_var.x)\n",
        "    for i in range(ip.SolCount):\n",
        "        ip.setParam('SolutionNumber', i)\n",
        "        sol = np.array(ip.xn).round().astype(bool)\n",
        "        if c @ sol < obj:\n",
        "            break\n",
        "        else:\n",
        "            solutions.append(sol)\n",
        "    return obj, np.stack(solutions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKThhWpJsc7X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ncCnLY4sdVI"
      },
      "source": [
        "## Data and dataset\n",
        "### Problems to be explored\n",
        "Please refer to the Dataset part for details.\n",
        "\n",
        "#### Set Cover Problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KPIeIR4bsLgs"
      },
      "outputs": [],
      "source": [
        "from problems.setcover import *\n",
        "problem_func = generate_setcover\n",
        "encoding_func = setcover_encoding\n",
        "def size_func():\n",
        "    return {'nrows': 20, 'ncols': 40, 'density': np.random.uniform(0.1, 0.25)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0rlaO539Gu_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhS8_QTAsmmr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6vQyn6PvVFN"
      },
      "source": [
        "#### Verify the problem size\n",
        "- $A\\in \\mathbb{R}^{m\\times n}$\n",
        "- $b\\in \\mathbb{R}^m$\n",
        "- $c\\in \\mathbb{R}^n$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NwmeD98pvUH9"
      },
      "outputs": [],
      "source": [
        "def valid_dim(A, b, c, x = None):\n",
        "    m, n = A.shape \n",
        "    return b.shape == (m, ) and c.shape == (n,) and (x is None or x.shape == (n, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6UlDt8rsm6L"
      },
      "source": [
        "### Encoding of the problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xL0XgfospHp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo4h6mANsp0h"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XsEyGpKetWtw"
      },
      "outputs": [],
      "source": [
        "class BinaryIPDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Generate a problem instance with a random, non-terminal assignment\n",
        "    '''\n",
        "    def __init__(self, *, problem_func, size_func, assignment_func, encoding_func):\n",
        "        '''\n",
        "        problem_func: problem to be explored\n",
        "        size_func: size as a input parameter to the problem_func\n",
        "        problem_func(**size_func()) should work\n",
        "        \n",
        "        assignment_func: used to generate an assignemnt for the given problem\n",
        "        encoding_func: \n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.problem_func = problem_func\n",
        "        self.size_func = size_func\n",
        "        self.assignment_func = assignment_func\n",
        "        self.encoding_func = encoding_func\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError # not needed\n",
        "\n",
        "    def __getitem__(self, index:int):\n",
        "        # get a random instance in the class of problem with a random size\n",
        "        A, b, c = self.problem_func(**self.size_func())\n",
        "\n",
        "        # get a meaningful but random variable assignemnt\n",
        "        x_assignment = self.assignment_func(A, b, c)\n",
        "        \n",
        "        # get a random intermediate solution\n",
        "        _, solutions = gurobi_optimal(A, b, c)\n",
        "\n",
        "        difference = np.logical_xor(x_assignment, solutions)\n",
        "        dist = difference.sum(axis = 1)\n",
        "        min_dist_difference = torch.tensor(difference[dist == dist.min()])\n",
        "        \n",
        "        u, v, e = self.encoding_func(A, b, c, x_assignment)\n",
        "\n",
        "        return u, v, e, min_dist_difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEx5X_A8ugzV"
      },
      "source": [
        "#### Totally random assignment function\n",
        "\n",
        "But first draw uniformly the number of 1-valued variables, then the number of variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E9UAuK_DvMwk"
      },
      "outputs": [],
      "source": [
        "def random_assignment(A, b, c):\n",
        "    m, n = A.shape\n",
        "    num_ones = np.random.randint(n + 1)\n",
        "    assignment = np.zeros_like(c, dtype = bool)\n",
        "    assignment[np.random.choice(n, size = num_ones, replace = False)] = True\n",
        "    return assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dense_stack(*args, output_relu = True):\n",
        "    seq = nn.Sequential()\n",
        "    for i in range(1, len(args)):\n",
        "        seq.add_module(f'dense {i-1}', nn.Linear(args[i-1], args[i]))\n",
        "        if i < len(args) - 1 or output_relu:\n",
        "            seq.add_module(f'relu {i-1}', nn.ReLU())\n",
        "    return seq\n",
        "\n",
        "class HalfConvolution(nn.Module):\n",
        "    '''\n",
        "    input: a bipartite graph\n",
        "    u: features of the nodes on the same side (U, F) \n",
        "    v: features of the nodes on the opposite side (V, G)\n",
        "    e: featuers of of the edges (V, U, H)\n",
        "\n",
        "    g_args (F+G+H, ..., D)\n",
        "    f_args (F+D, ...)\n",
        "    '''\n",
        "    def __init__(self, *, f_args, g_args):\n",
        "        super().__init__()\n",
        "        self.g = dense_stack(*g_args)\n",
        "        self.f = dense_stack(*f_args)\n",
        "    def forward(self, u, v, e):\n",
        "        U, F = u.shape\n",
        "        V, G = v.shape\n",
        "        _, _, H = e.shape\n",
        "        assert e.shape == (V, U, H)\n",
        "        \n",
        "        g_out, _ = self.g(torch.cat([u.unsqueeze(-3).expand(V, U, F), v.unsqueeze(-2).expand(V, U, G), e], axis = -1)).max(axis = -3) # (V, U, D) to (U, D)\n",
        "        out = self.f(torch.cat([u, g_out], axis = -1))\n",
        "        return out\n",
        "\n",
        "class HalfConvolutionModel(nn.Module):\n",
        "    '''\n",
        "    input: same as HalfConvolution\n",
        "\n",
        "    variable_args \n",
        "    constraits_args\n",
        "    final_args\n",
        "    '''\n",
        "    def __init__(self, *, v_feats: int, c_feats: int, e_feats: list, g_hidden_neurons: list, f_hidden_neurons: list, out_neurons: list):\n",
        "        super().__init__()\n",
        "        self.half_conv = HalfConvolution(\n",
        "            g_args = [v_feats+c_feats+e_feats] + g_hidden_neurons,\n",
        "            f_args = [v_feats + g_hidden_neurons[-1]] + f_hidden_neurons\n",
        "        )\n",
        "        self.out = dense_stack(f_hidden_neurons[-1], *out_neurons, output_relu = False)\n",
        "    \n",
        "    def forward(self, u, v, e):\n",
        "        out = self.half_conv(u, v, e)  \n",
        "        return self.out(out)\n",
        "    \n",
        "    def predict(self, A, b, c, x):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "### Set up the dataset and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = HalfConvolutionModel(v_feats= 2, c_feats= 5, e_feats=1, g_hidden_neurons=[64, 64], f_hidden_neurons=[64, 64], out_neurons=[64, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eP2gcada-6Xj"
      },
      "outputs": [],
      "source": [
        "ds = BinaryIPDataset(problem_func = problem_func, size_func = size_func, assignment_func = random_assignment, encoding_func= encoding_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = np.sqrt(0.1))\n",
        "Loss = torch.nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "\n",
        "num_batch, batch_size = 100, 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0, loss: 0.689\n",
            "Batch 1, loss: 0.690\n",
            "Batch 2, loss: 0.675\n",
            "Batch 3, loss: 0.664\n",
            "Batch 4, loss: 0.619\n",
            "Batch 5, loss: 0.580\n",
            "Batch 6, loss: 0.534\n",
            "Batch 7, loss: 0.491\n",
            "Batch 8, loss: 0.487\n",
            "Batch 9, loss: 0.490\n",
            "Batch 10, loss: 0.455\n",
            "Batch 11, loss: 0.438\n",
            "Batch 12, loss: 0.443\n",
            "Batch 13, loss: 0.417\n",
            "Batch 14, loss: 0.421\n",
            "Batch 15, loss: 0.449\n",
            "Batch 16, loss: 0.428\n",
            "Batch 17, loss: 0.414\n",
            "Batch 18, loss: 0.422\n",
            "Batch 19, loss: 0.385\n",
            "Batch 20, loss: 0.442\n",
            "Batch 21, loss: 0.354\n",
            "Batch 22, loss: 0.356\n",
            "Batch 23, loss: 0.374\n",
            "Batch 24, loss: 0.383\n",
            "Batch 25, loss: 0.397\n",
            "Batch 26, loss: 0.369\n",
            "Batch 27, loss: 0.359\n",
            "Batch 28, loss: 0.393\n",
            "Batch 29, loss: 0.390\n",
            "Batch 30, loss: 0.344\n",
            "Batch 31, loss: 0.364\n",
            "Batch 32, loss: 0.352\n",
            "Batch 33, loss: 0.344\n",
            "Batch 34, loss: 0.341\n",
            "Batch 35, loss: 0.369\n",
            "Batch 36, loss: 0.386\n",
            "Batch 37, loss: 0.346\n",
            "Batch 38, loss: 0.369\n",
            "Batch 39, loss: 0.322\n",
            "Batch 40, loss: 0.321\n",
            "Batch 41, loss: 0.338\n",
            "Batch 42, loss: 0.388\n",
            "Batch 43, loss: 0.356\n",
            "Batch 44, loss: 0.380\n",
            "Batch 45, loss: 0.328\n",
            "Batch 46, loss: 0.377\n",
            "Batch 47, loss: 0.341\n",
            "Batch 48, loss: 0.331\n",
            "Batch 49, loss: 0.369\n",
            "Batch 50, loss: 0.333\n",
            "Batch 51, loss: 0.374\n",
            "Batch 52, loss: 0.377\n",
            "Batch 53, loss: 0.327\n",
            "Batch 54, loss: 0.353\n",
            "Batch 55, loss: 0.343\n",
            "Batch 56, loss: 0.359\n",
            "Batch 57, loss: 0.340\n",
            "Batch 58, loss: 0.342\n",
            "Batch 59, loss: 0.338\n",
            "Batch 60, loss: 0.374\n",
            "Batch 61, loss: 0.382\n",
            "Batch 62, loss: 0.338\n",
            "Batch 63, loss: 0.294\n",
            "Batch 64, loss: 0.342\n",
            "Batch 65, loss: 0.385\n",
            "Batch 66, loss: 0.340\n",
            "Batch 67, loss: 0.343\n",
            "Batch 68, loss: 0.382\n",
            "Batch 69, loss: 0.342\n",
            "Batch 70, loss: 0.366\n",
            "Batch 71, loss: 0.343\n",
            "Batch 72, loss: 0.378\n",
            "Batch 73, loss: 0.356\n",
            "Batch 74, loss: 0.370\n",
            "Batch 75, loss: 0.346\n",
            "Batch 76, loss: 0.329\n",
            "Batch 77, loss: 0.355\n",
            "Batch 78, loss: 0.364\n",
            "Batch 79, loss: 0.326\n",
            "Batch 80, loss: 0.364\n",
            "Batch 81, loss: 0.362\n",
            "Batch 82, loss: 0.351\n",
            "Batch 83, loss: 0.341\n",
            "Batch 84, loss: 0.332\n",
            "Batch 85, loss: 0.405\n",
            "Batch 86, loss: 0.349\n",
            "Batch 87, loss: 0.340\n",
            "Batch 88, loss: 0.346\n",
            "Batch 89, loss: 0.370\n",
            "Batch 90, loss: 0.323\n",
            "Batch 91, loss: 0.337\n",
            "Batch 92, loss: 0.314\n",
            "Batch 93, loss: 0.338\n",
            "Batch 94, loss: 0.314\n",
            "Batch 95, loss: 0.356\n",
            "Batch 96, loss: 0.366\n",
            "Batch 97, loss: 0.340\n",
            "Batch 98, loss: 0.342\n",
            "Batch 99, loss: 0.310\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_batch):\n",
        "    optimizer.zero_grad()\n",
        "    loss = torch.tensor(0.0)\n",
        "    for j in range(batch_size):\n",
        "        u, v, e, min_dist_diff = ds[j]\n",
        "        N_sols, _ = min_dist_diff.shape\n",
        "\n",
        "        scores = model(u, v, e).t().expand(N_sols, -1) # (n, 1) -> (1, n) -> (N_sols, n)\n",
        "\n",
        "        _loss = Loss(input = scores, target = min_dist_diff.float()).mean(axis = 1)\n",
        "        loss = loss + _loss.min() / batch_size \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    print(f'Batch {i}, loss: {loss.item():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test set generating\n",
        "def generate_test(len_test=1026, problem_func=problem_func):\n",
        "    problems, objectives = [], []\n",
        "    for i in range(len_test):\n",
        "        A, b, c = problem_func(**size_func())\n",
        "        problems.append((A, b, c))\n",
        "        obj, _ = gurobi_optimal(A, b, c)\n",
        "        objectives.append(obj)\n",
        "    \n",
        "    return problems, objectives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimator(A, b, c, x, encoding_func=encoding_func):\n",
        "    if model.training:\n",
        "        model.eval()\n",
        "    with torch.no_grad():\n",
        "        u, v, e = encoding_func(A, b, c, x)\n",
        "        u, v, e = torch.tensor(u).float(), torch.tensor(v).float(), torch.tensor(e).float()\n",
        "        scores = model(u, v, e).detach().numpy().flatten()\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import queue\n",
        "def tree_search(A, b, c, objective, estimator, threshold=0.5, max_children=2, num_step_limit = np.inf):\n",
        "    m, n = A.shape\n",
        "    assert b.shape == (m, ) and c.shape == (n, )\n",
        "    tree = queue.Queue()\n",
        "\n",
        "    step_count = 0\n",
        "    while step_count < num_step_limit:  \n",
        "        if tree.empty():\n",
        "            x = np.zeros_like(c, dtype = bool)\n",
        "        else:\n",
        "            x = queue.get()\n",
        "\n",
        "        score = estimator(A, b, c, x)\n",
        "        score[score < threshold] = 0\n",
        "        indices = np.argsort(a, axis=-1, kind='quicksort', order=None)[:max_children]\n",
        "        for index in indices:\n",
        "            if score[index] == 0:\n",
        "                break\n",
        "            else: \n",
        "                x[index] = ~x[index]\n",
        "                valid = (A @ x <= b).all()\n",
        "                if valid:\n",
        "                    tree.put(x)\n",
        "\n",
        "        step_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tree-search.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
      "name": "python38864bitpytorchconda7790f7e226db4d799e23e7669a0b2db2"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
